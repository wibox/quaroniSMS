{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-28 16:30:22.509642: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-04-28 16:30:22.541742: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-04-28 16:30:23.197046: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-28 16:30:23.197102: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-28 16:30:23.197110: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"]=\"1\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "from preprocessing import *\n",
    "import utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf formatted_data\n",
    "! rm -rf complete_test_data\n",
    "! rm -rf complete_training_data\n",
    "! rm -rf complete_validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf saved_models\n",
    "! rm -rf tflite_models\n",
    "! rm -rf zipped_models\n",
    "! rm -rf results/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_time = 3 #seconds\n",
    "window_length = 2 #seconds\n",
    "overlap_size = 1 #seconds\n",
    "strategy = \"window\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading audio files informations...\n",
      "Found 399 audio files.\n",
      "Cropping audio files to 3 seconds before and after passing time. Saving to ./formatted_data/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/399 [00:00<?, ?it/s]/home/sbenghus/Desktop/quaroniSMS/preprocessing.py:117: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  input_audiofile = siw.read(audio_path)\n",
      "100%|██████████| 399/399 [00:01<00:00, 290.36it/s]\n"
     ]
    }
   ],
   "source": [
    "df = DatasetFormatter(crop_time=crop_time, window_length=window_length, overlap_size=overlap_size)\n",
    "df.format_dataset(strategy=strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir complete_training_data\n",
    "! mkdir complete_validation_data\n",
    "! mkdir complete_test_data\n",
    "\n",
    "! mv formatted_data/formatted_VWPassat/* complete_test_data\n",
    "! mv formatted_data/formatted_CitroenC4Picasso/* complete_validation_data\n",
    "! mv formatted_data/*/* complete_training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: saved_models/*\n",
      "zsh:1: no matches found: tflite_models/*\n"
     ]
    }
   ],
   "source": [
    "! rm -rf saved_models/*\n",
    "! rm -rf tflite_models/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible configurations found: 1\n"
     ]
    }
   ],
   "source": [
    "PARAMS = {\n",
    "    'frame_length_in_s': [0.04], \n",
    "    'frame_step_in_s': [0.02], \n",
    "    'num_mel_bins': [20], \n",
    "    'lower_frequency': [1000],\n",
    "    'upper_frequency': [7000], \n",
    "    'batch_size': [5], \n",
    "    'epochs': [1], \n",
    "    'initial_learning_rate': [0.01], \n",
    "    'end_learning_rate': [1.e-5], \n",
    "    'num_mfccs_features' : [-1], \n",
    "    'alpha': [0.15], \n",
    "    'num_hidden_layers':[5]\n",
    "}\n",
    "\n",
    "configurations = {\"configurations\": []}\n",
    "my_configs = ParameterGrid(PARAMS)\n",
    "for config in my_configs:\n",
    "    configurations[\"configurations\"].append(config)\n",
    "\n",
    "print(\"Possible configurations found: {}\".format(len(configurations[\"configurations\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-28 16:30:27.882521: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-04-28 16:30:27.882557: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configuration: \n",
      " {'alpha': 0.15, 'batch_size': 5, 'end_learning_rate': 1e-05, 'epochs': 1, 'frame_length_in_s': 0.04, 'frame_step_in_s': 0.02, 'initial_learning_rate': 0.01, 'lower_frequency': 1000, 'num_hidden_layers': 5, 'num_mel_bins': 20, 'num_mfccs_features': -1, 'upper_frequency': 7000}\n",
      "(5, 32, 32, 1)\n",
      "tf.Tensor([1 0 1 1 0], shape=(5,), dtype=int64)\n",
      "69/69 [==============================] - 83s 1s/step - loss: 0.9870 - sparse_categorical_accuracy: 0.4956 - val_loss: 1.0426 - val_sparse_categorical_accuracy: 0.4783\n",
      "Saving model to ./saved_models/\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_models/cnn_0/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_models/cnn_0/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting model to TF-Lite format...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-28 16:32:00.221850: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n",
      "2023-04-28 16:32:00.221878: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n",
      "2023-04-28 16:32:00.607399: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1918] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following Select TFop(s):\n",
      "Flex ops: FlexTensorListReserve, FlexTensorListSetItem, FlexTensorListStack\n",
      "Details:\n",
      "\ttf.TensorListReserve(tensor<2xi32>, tensor<i32>) -> (tensor<!tf_type.variant<tensor<?x128xf32>>>) : {device = \"\"}\n",
      "\ttf.TensorListSetItem(tensor<!tf_type.variant<tensor<?x128xf32>>>, tensor<i32>, tensor<?x128xf32>) -> (tensor<!tf_type.variant<tensor<?x128xf32>>>) : {device = \"\"}\n",
      "\ttf.TensorListStack(tensor<!tf_type.variant<tensor<?x128xf32>>>, tensor<2xi32>) -> (tensor<1x?x128xf32>) : {device = \"\", num_elements = 1 : i64}\n",
      "See instructions: https://www.tensorflow.org/lite/guide/ops_select\n",
      "INFO: Created TensorFlow Lite delegate for select TF ops.\n",
      "INFO: TfLiteFlexDelegate delegate: 2 nodes delegated out of 13 nodes with 2 partitions.\n",
      "\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TF-Lite model to ./tflite_models/\n",
      "Zipping and saving the model to ./zipped_models/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:40<00:00, 100.92s/it]\n"
     ]
    }
   ],
   "source": [
    "header = ','.join(k for k in PARAMS.keys()) + \",\" + ','.join([\n",
    "    \"accuracy\", \"avg_preprocessing_latency\", \"avg_model_latency\", \\\n",
    "         \"median_total_latency\", \"model_size\", \"compressed_model_size\", \"model_id\"\n",
    "         ])\n",
    "\n",
    "utilities._log_header_to_csv(filename=f\"{strategy}_lstm_results.csv\", header=header)\n",
    "\n",
    "train_file_ds = tf.data.Dataset.list_files(['complete_training_data/*.wav'])\n",
    "validation_file_ds = tf.data.Dataset.list_files(['complete_validation_data/*.wav'])\n",
    "test_file_ds = tf.data.Dataset.list_files(['complete_test_data/*.wav'])\n",
    "\n",
    "for idx in tqdm(range(len(configurations[\"configurations\"]))):\n",
    "\n",
    "    config = configurations[\"configurations\"][idx]\n",
    "\n",
    "    print(\"Using configuration: \\n\", config)\n",
    "\n",
    "    MEL_LOG_ARGS = {\n",
    "        'frame_length_in_s': config['frame_length_in_s'],\n",
    "        'frame_step_in_s': config['frame_step_in_s'],\n",
    "        'num_mel_bins': config['num_mel_bins'],\n",
    "        'lower_frequency': config['lower_frequency'],\n",
    "        'upper_frequency': config['upper_frequency']\n",
    "    }\n",
    "    TRAINING_ARGS= {\n",
    "        'batch_size': config['batch_size'],\n",
    "        'epochs': config['epochs'],\n",
    "        'initial_learning_rate': config['initial_learning_rate'],\n",
    "        'end_learning_rate': config['end_learning_rate']\n",
    "    }\n",
    "\n",
    "    batch_size = TRAINING_ARGS['batch_size']\n",
    "    epoch = TRAINING_ARGS['epochs']\n",
    "\n",
    "    get_frozen_log_mel_spectrogram = partial(get_log_mel_spectrogram, **MEL_LOG_ARGS)\n",
    "    train_mel_ds = train_file_ds.map(get_frozen_log_mel_spectrogram)\n",
    "\n",
    "    for spectrogram, label in train_mel_ds.take(1):\n",
    "        SHAPE = spectrogram.shape\n",
    "\n",
    "    def preprocess_with_resized_mel(filename):\n",
    "        log_mel_spectrogram, label = get_frozen_log_mel_spectrogram(filename)\n",
    "        log_mel_spectrogram.set_shape(SHAPE)\n",
    "        mfccs = tf.signal.mfccs_from_log_mel_spectrograms(log_mel_spectrogram)[..., :]\n",
    "        mfccs = tf.expand_dims(mfccs, -1)\n",
    "        mfccs = tf.image.resize(mfccs, [32, 32])\n",
    "        label_id = tf.argmax(label == LABELS)\n",
    "\n",
    "        return mfccs, label_id\n",
    "\n",
    "    train_ds = train_file_ds.map(preprocess_with_resized_mel).batch(batch_size)\n",
    "    validation_ds = validation_file_ds.map(preprocess_with_resized_mel).batch(batch_size)\n",
    "\n",
    "    for example_batch, example_labels in train_ds.take(1):\n",
    "        print(example_batch.shape)\n",
    "        print(example_labels)\n",
    "\n",
    "    model = utilities.get_lstm()\n",
    "\n",
    "    model_for_pruning, callbacks = utilities.compile_pruning_model(\n",
    "        model = model,\n",
    "        epoch = epoch,\n",
    "        dim = len(train_ds),\n",
    "        i_lr = TRAINING_ARGS['initial_learning_rate'],\n",
    "        e_lr = TRAINING_ARGS['end_learning_rate']\n",
    "    )\n",
    "\n",
    "    history = model_for_pruning.fit(train_ds, epochs=epoch, validation_data=validation_ds, callbacks=callbacks)\n",
    "\n",
    "    # computing statistics\n",
    "    traning_loss, \\\n",
    "        training_accuracy, \\\n",
    "            val_loss, \\\n",
    "                val_accuracy = utilities.get_model_statistics(history = history)\n",
    "\n",
    "    MODEL_NAME, ZIPPED_MODEL_NAME = utilities.convert_zip_save_model(model=model_for_pruning, idx=idx, network_type=\"lstm\")\n",
    "\n",
    "    # performing inference\n",
    "    interpreter = tf.lite.Interpreter(model_path=f'tflite_models/{MODEL_NAME}.tflite')\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    avg_preprocessing_latency = 0.0\n",
    "    avg_model_latency = 0.0\n",
    "    latencies = []\n",
    "    accuracy = 0.0\n",
    "\n",
    "    start_map = time()\n",
    "    mapped_test_ds = test_file_ds.map(get_frozen_log_mel_spectrogram)\n",
    "    end_map =  time()\n",
    "    avg_map_time = (end_map - start_map)/len(test_file_ds)\n",
    "\n",
    "    for log_mel_spectrogram, true_label in mapped_test_ds:\n",
    "\n",
    "        start_preprocess = time()\n",
    "\n",
    "        mfccs = tf.signal.mfccs_from_log_mel_spectrograms(log_mel_spectrogram)[..., :config['num_mfccs_features']]\n",
    "        mfccs = tf.expand_dims(mfccs, 0)\n",
    "        mfccs = tf.expand_dims(mfccs, -1)\n",
    "        mfccs = tf.image.resize(mfccs, [32, 32])\n",
    "        mfccs = tf.squeeze(mfccs, axis=-1)\n",
    "        end_preprocess = time()\n",
    "        \n",
    "        interpreter.set_tensor(input_details[0]['index'], mfccs) \n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "        end_inference = time()\n",
    "\n",
    "        top_index = np.argmax(output[0])\n",
    "        predicted_label = LABELS[top_index]\n",
    "\n",
    "        accuracy += true_label.numpy().decode() == predicted_label\n",
    "        avg_preprocessing_latency += avg_map_time + (end_preprocess - start_preprocess)\n",
    "        avg_model_latency += end_inference - end_preprocess\n",
    "        latencies.append(end_inference - start_preprocess)\n",
    "\n",
    "    accuracy /= len(mapped_test_ds)\n",
    "    avg_preprocessing_latency /= len(mapped_test_ds)\n",
    "    avg_model_latency /= len(mapped_test_ds)\n",
    "    median_total_latency = np.median(latencies)\n",
    "\n",
    "    model_size = os.path.getsize(f'tflite_models/{MODEL_NAME}.tflite')\n",
    "    compressed_model_size = os.path.getsize(f\"zipped_models/{ZIPPED_MODEL_NAME}.zip\")\n",
    "\n",
    "    content = f\"{config['frame_length_in_s']},\\\n",
    "        {config['frame_step_in_s']},{config['num_mel_bins']},\\\n",
    "            {config['lower_frequency']},{config['upper_frequency']},\\\n",
    "                {config['batch_size']},{config['epochs']},{config['initial_learning_rate']},\\\n",
    "                    {config['end_learning_rate']},{config['num_mfccs_features']},\\\n",
    "                        {config['alpha']},{config['num_hidden_layers']},\\\n",
    "                            {100 * accuracy:.3f},{1000 * avg_preprocessing_latency:.1f},\\\n",
    "                            {1000 * avg_model_latency:.1f},{1000 * median_total_latency:.1f},\\\n",
    "                                {model_size / 2 ** 10:.1f},{compressed_model_size / 2 ** 10:.1f},{idx}\\n\"\n",
    "\n",
    "    utilities._log_output_to_csv(filename=f\"{strategy}_lstm_results.csv\", content=content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
